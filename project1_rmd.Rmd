---
title: "Project 1"
author: "Nancie Kung, Calvin Raab, David Collier and Eitan Shimonovitz"
date: "4/21/2021"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load libraries and import the data. 
```{r, message = FALSE, warning = FALSE}
library(readr)
library(dplyr)
library(leaps)
library(ggplot2)
library(reshape2)
library(scales)
library(corrplot)
library(car)
library(stargazer)
library(lmtest)
library(broom)
library(AER)
library(caret)

Fat_Supply_Quantity_Data <- read_csv("Fat_Supply_Quantity_Data.csv")
```

Data cleaning process.
```{r, warning = FALSE}
#select columns that are not filled with zeros
Fat_Supply_data <- Fat_Supply_Quantity_Data %>% select(Country, `Animal Products`, 
  `Animal fats`, `Cereals - Excluding Beer`, Eggs, `Fish, Seafood`, `Fruits - Excluding Wine`, 
  Meat, `Milk - Excluding Butter`, Offals, Oilcrops, Pulses, Spices, `Starchy Roots`, 
  Stimulants, Treenuts, `Vegetal Products`, `Vegetable Oils`, Vegetables, Obesity, 
  Undernourished, Population, Confirmed, Deaths)

Fat_Supply_data <- Fat_Supply_data[Fat_Supply_data$Deaths > 0,]
Fat_Supply_data <- Fat_Supply_data[!is.na(Fat_Supply_data$Deaths),]


Fat_Supply_data$Undernourished[Fat_Supply_data$Undernourished == "<2.5"] <- 2.5
Fat_Supply_data$Undernourished <- as.numeric(Fat_Supply_data$Undernourished)

# replace NAs in Obesity and Undernourished with the median values
Fat_Supply_data$Obesity[is.na(Fat_Supply_data$Obesity)] <- 
  median(Fat_Supply_data$Obesity, na.rm=TRUE)
Fat_Supply_data$Undernourished[is.na(Fat_Supply_data$Undernourished)] <- 
  median(Fat_Supply_data$Undernourished, na.rm=TRUE)

data <- Fat_Supply_data

# Here is a dataset that includes the parameters found in backAIC, 
# along with: Country, Population, Confirmed, and Deaths (Using For Analysis) 
backAICdata.plus <- data_frame(data$`Country`, data$`Animal fats`, 
    data$`Cereals - Excluding Beer`, data$`Fruits - Excluding Wine`, 
    data$`Oilcrops`, data$`Pulses`, data$`Spices`, data$`Starchy Roots`, 
    data$Stimulants, data$Treenuts, data$`Vegetal Products`, data$`Vegetable Oils`, 
    data$Vegetables, data$Obesity, data$Undernourished, data$Population, 
    data$Confirmed, data$Deaths)

names(backAICdata.plus) <- c("Country", "Animal_Fats", "Cereals", "Fruits", 
   "Oilcrops", "Pulses", "Spices", "Starchy_Roots", "Stimulants", "Treenuts", 
   "Vegetal Products", "Vegetable_Oils", "Vegetables", "Obesity", "Undernourished", 
   "Population", "Confirmed", "Deaths")


```

### Part 1 
Provide a descriptive analysis of your variables. This should include histograms and fitted distributions, correlation plot, boxplots, scatterplots, and statistical summaries (e.g., the five-number summary). All figures must include comments.

Columns in dataset:  
* Fat Supply Measures - Average percentage (out of 100) of fat in diet that comes from each category of food
    - Categories included: Animal_Fats, Cereals, Fruits, Oilcrops, Pulses, Spices, Starchy_Roots, Stimulants, Treenuts, Vegetal Products, Vegetable_oils, and Vegetables
    
* Population Health Measures - Percentage of the population that falls into each category
    - Obesity and Undernourished
    
* Population and COVID Measures
    - Population - Population of country
    - Confirmed - Percentage of population with a confirmed positive test for COVID-19
    - Deaths - Percentage of population that died from COVID-19
    

```{r}
# create a boxplot of food categories


# melt the data into long form
fat_data <- melt(backAICdata.plus[,1:13], id = "Country")

# create boxplots
ggplot(fat_data, aes(x = variable, y = value)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(labels = label_percent(scale = 1)) +
  xlab("Food Category") +
  ylab("Percentage of Fat") +
  ggtitle("Boxplot of Percentage of Fat in Diet from Food Categories")
```

From the above boxplots, we can see that Vegetal Products and Vegetable Oils are major sources of fat for all countries, while the average values for other categories are low. We can also see that Oilcrops has a relatively large amount of high outliers compared to other groups.  

```{r}
# correlation plot of all variables
library(corrplot)
corrplot(cor(backAICdata.plus[,-1]), method = "circle")
```

From the above correlation plot, we can see some interesting correlations between some food groups, such as between Vetegal Procucts and Animal Fats. We also see that Obesity and Undernourished are strongly negatively correlated, which makes sense, and that there is a very high correlation between Confirmed Cases and Deaths, which is also to be expected. 


```{r}
# five number summaries for each numeric column

apply(backAICdata.plus[,-1], 2, summary)


```


```{r}
library(ggplot2)
# Percentage of confirmed cases by country
ggplot(data = backAICdata.plus, aes(x=Country, y=Confirmed, label= Country)) + 
  geom_point() + scale_x_discrete(guide = guide_axis(check.overlap = TRUE))
# Percentage of deaths by country
ggplot(data = backAICdata.plus, aes(x=Country, y=Deaths, label= Country)) + 
  geom_point() + scale_x_discrete(guide = guide_axis(check.overlap = TRUE))
```

The above scatterplot details the percentage of confirmed cases in each country. Here it can be seen that the majority of cases lie between zero and 2.5%. From this graphic it can be seen that the highest percentage of covid cases is above 10%. 


### Part 2
Estimate a multiple linear regression model that includes all the main effects only (i.e., no interactions nor higher order terms). We will use this model as a baseline. Comment on the statistical and economic significance of your estimates. Also, make sure to provide an interpretation of your estimates.
```{r}

# To reduce the number of columns in our dataset to a more workable amount, 
# we used backward selection with AIC to pick the predictors we wanted to include. 
# The dataset used for question one used only the selected columns.

model_all <- lm(data$Deaths ~ data$`Animal Products` + data$`Animal fats` + 
  data$`Cereals - Excluding Beer` + data$Eggs + data$`Fish, Seafood` + 
    data$`Fruits - Excluding Wine` + data$Meat + data$`Milk - Excluding Butter` + 
    data$Offals + data$Oilcrops + data$Pulses + data$Spices + data$`Starchy Roots` + 
    data$Stimulants + data$Treenuts + data$`Vegetal Products` + 
    data$`Vegetable Oils` + data$Vegetables + data$Obesity + data$Undernourished + data$Population)

#summary(model_all)

n <- length(data$Deaths)
```

```{r, message = FALSE, results = FALSE}
backAIC <- step(model_all ,direction="backward", data=data)
```

```{r}
# Baseline Model
summary(backAIC)
```

As can be seen in the model output above, all food categories are statistically significant at the $\alpha = .05$ level. Obesity is also a statistically significant predictor, though Undernourished is surprisingly not statistically significant. The magnitude of the estimates for the food categories is roughly the same, with a 1% increase in fat from each food category leading to a .17 - .19 percent change in expected death rate from COVID-19. What is interesting is that Vegetal Products is the only statistically significant predictor with a negative coefficient, while all other food categories are positive. An increase of 1% in population obesity leads to an increase in .001% of expected COVID-19 death rate.  

### Part 3
Identify if there are any outliers, high leverage, and or influential observations worth removing. If so, remove them but justify your reason for doing so and re-estimate your model.  

We examined for unusual observations using the base residual plot as well as influence and residual plots from the olsrr package to get a comprehensive overview of which points may be unusual observations.  

```{r}
library(olsrr)

par(mfrow=c(2,2))
plot(backAIC)
influencePlot(backAIC, id=list(n=3)) 
ols_plot_resid_lev(backAIC)
```
  
The following observations were identified by both plots as unusual:  
* High leverage = 120 (Rwanda), 90 (Maldives), 47 (Ethiopia)  
* Outlier = 103 (New Zealand), 113 (Peru)
* Influential = 19 (Bosnia/Herzegovina)

We next created new models that used reduced versions of the dataset, removing each type of unusual observation.
```{r}
# Remove the unusual observations from the data with slice 
no_highleverage <- backAICdata.plus %>% slice(-c(120,90,47))
no_influential <- backAICdata.plus %>% slice(-19)
no_outlier <- backAICdata.plus %>% slice(-c(103,113))
no_unusual_observations <- backAICdata.plus %>% slice(-c(120,90,47,19,103,113))
```

```{r mylatextable, results = "asis"}
# Create new models without the unusual observations
mod0 <- lm(Deaths~Animal_Fats+Cereals+Fruits+Oilcrops+Pulses+Spices+
             Starchy_Roots+Stimulants+Treenuts+`Vegetal Products`+
             Vegetable_Oils+Vegetables+Obesity, data=backAICdata.plus)
mod1 <- lm(Deaths~Animal_Fats+Cereals+Fruits+Oilcrops+Pulses+Spices+
             Starchy_Roots+Stimulants+Treenuts+`Vegetal Products`+
             Vegetable_Oils+Vegetables+Obesity, data=no_highleverage)
mod2 <- lm(Deaths~Animal_Fats+Cereals+Fruits+Oilcrops+Pulses+Spices+
             Starchy_Roots+Stimulants+Treenuts+`Vegetal Products`+
             Vegetable_Oils+Vegetables+Obesity, data=no_influential)
mod3 <- lm(Deaths~Animal_Fats+Cereals+Fruits+Oilcrops+Pulses+Spices+
             Starchy_Roots+Stimulants+Treenuts+`Vegetal Products`+
             Vegetable_Oils+Vegetables+Obesity, data=no_outlier)
mod4 <- lm(Deaths~Animal_Fats+Cereals+Fruits+Oilcrops+Pulses+Spices+
             Starchy_Roots+Stimulants+Treenuts+`Vegetal Products`+
             Vegetable_Oils+Vegetables+Obesity, data=no_unusual_observations)

stargazer(mod0, mod1, mod2, mod3, mod4, object.names = TRUE, 
          title = "Regression Model Results", column.labels = 
            c("Original", "No High Leverage", "No Influential", 
              "No Oultlier", "No Unusual Observations"), type = 'latex', 
          header = FALSE, no.space = TRUE, single.row = TRUE, 
          font.size = "small", column.sep.width = "-15pt")
```
  
As can be seen in the table above, the R-squared of the model that removes all outliers ahigh leverage points is a noticeable improvement over the base model including all data points. We noticed that the estimates for the coefficients did not change much between models, indicating that while the outliers were negatively affecting the accuracy metrics of the model, they were not changing the estimates for the parameters themselves. We decided to remove the outliers and leverage points because we wanted to be able to compare future models against each other fairly, without the outliers influencing the accuracy metrics of those models.  

```{r}
# New model 
new_model_1 <- lm(Deaths~Animal_Fats+Cereals+Fruits+Oilcrops+Pulses+Spices+
                    Starchy_Roots+Stimulants+Treenuts+`Vegetal Products`+
                    Vegetable_Oils+Vegetables+Obesity, data=no_unusual_observations)
summary(new_model_1)
```


### Part 4
Use Mallows Cp for identifying which terms you will keep in the model (based on part 3) and also use the Boruta algorithm for variable selection. Based on the two results, determine which subset of predictors you will keep.

```{r, message=FALSE}
# Since Mallows CP has a lower number when testing our new_model_1, we will proceed with that model. 
ols_mallows_cp(new_model_1, model_all)
ols_mallows_cp(backAIC, model_all)


library(Boruta)

Bor.res <- Boruta(Deaths~Animal_Fats+Cereals+Fruits+Oilcrops+Pulses+Spices+
                    Starchy_Roots+Stimulants+Treenuts+`Vegetal Products`+
                    Vegetable_Oils+Vegetables+Obesity, data = no_unusual_observations, 
                  doTrace = 2 )




plot(Bor.res, xlab = "Attributes", xaxt = "n", main = "Boruta Algorithim")

lz<-lapply(1:ncol(Bor.res$ImpHistory),function(i)
Bor.res$ImpHistory[is.finite(Bor.res$ImpHistory[,i]),i])
names(lz) <- colnames(Bor.res$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(Bor.res$ImpHistory), cex.axis = 0.7)

# Testing to see which variables we want to remove
sorted_vars <- attStats(Bor.res)[order(-attStats(Bor.res)$meanImp),]
print(sorted_vars)
# We will reject: Stimulants, Treenuts, Starchy Roots, Vegetables, Spices, Fruits

# Our New Model 
new_model_2 <- lm(Deaths~Animal_Fats+Cereals+Oilcrops+Pulses+`Vegetal Products`+
                    Vegetable_Oils+Obesity, data=no_unusual_observations)
summary(new_model_2)
```

Based on Mallows Cp, we decided to go forward with new_model_1 as it had the lower number compared to the original model. We then further narrowed down our model with Baruta, which identified Stimulants, Treenuts, Starchy Roots, Vegetables, Spices, Fruits as unrobust.

### Part 5
Test for multicollinearity using VIF on the model from (4) . Based on the test, remove any appropriate variables, and estimate a new regression model based on these findings.
```{r}

vif(new_model_2) # We will remove any variable with a VIF over 5 to satisfy collinearity assumption


# Vegetal products has the highest VIF, so we remove that and repeat
vif_1 <- lm(Deaths~Animal_Fats+Cereals+Oilcrops+Pulses + Vegetable_Oils+Obesity, 
            data=no_unusual_observations)
vif(vif_1)
```

After removing Vegetal Products, all other predictors have a VIF under 5, so we will keep all of those.

```{r}
new_model_3 <- lm(Deaths~Animal_Fats+Cereals+Oilcrops+Pulses + Vegetable_Oils+
                    Obesity, data=no_unusual_observations)

#New Model 
summary(new_model_3)

```

In our new model, we can see that Animal Fats and Obesity are the only predictors that are statistically significant.  

### Part 6
For your model in part (5) plot the respective residuals vs. y_hat  and comment on your results.

```{r}
par(mfrow=c(2,2))
plot(new_model_3)
```

From the residuals vs fitted plot it can be seen that our residuals appear to spread out the greater our fitted value is. The red smoother runs close to zero which is a good thing. 

### Part 7
For your model in part (5) perform a RESET test and comment on your results.
 
```{r}
resettest(new_model_3, power = 2, type = "regressor")
```

Here we tested our model by testing our model against a quadratic. Our result is a p-value of 0.3703 which means that we fail to reject the null hypothesis of higher powers existing and should not consider higher powers in later version of our model.  

### Part 8
For your model in part (5) test for heteroskedasticity and comment on your results. If you identify heteroskedasticy, make sure to account for it before moving on to (9).

Below we will test for heteroskedacity using the ncvTest and bptest. 
```{r}
# Non-constant error variance: Ho: variance = constant 
ncvTest(new_model_3) # Reject Ho

# BP test 
bptest(new_model_3) #Reject Ho
```

From the above tests it can be seen that heteroskedacity is present in our data, as the p-value for both tests is essentially zero. In order to account for that we will now run our model with robust white standard errors. Here our new standard errors can be found. 

```{r}
cov1 <- hccm(new_model_3, type = "hc1")
#Have our model account for those errors. 
new_model_3_adjusted <- coeftest(new_model_3, vcov. = cov1)
tidy(new_model_3_adjusted)
```

The above table has the adjusted standard errors for the estimates when heterskedasticity is accounted for. We can see that Pulses now becomes statistically significant when using the adjusted standard errors despite not being significant earlier.  

### Part 9
Estimate a model based on all your findings that also includes interaction terms (if appropriate) and if needed, any higher power terms. Comment on the performance of this model compared to your other models. Make sure to use AIC and BIC for model comparison.
```{r}
# Our RESET test suggested there is no existence of higher power terms,
# so we will not test for those

# Testing for interaction
interaction_terms <- lm(Deaths~Animal_Fats+ Cereals + Oilcrops + Pulses + 
  Vegetable_Oils + Obesity + Animal_Fats:Cereals + Animal_Fats:Oilcrops +
    Animal_Fats:Pulses + Animal_Fats:Vegetable_Oils + Animal_Fats:Obesity + 
    Cereals:Oilcrops + Cereals:Pulses + 
    Cereals:Vegetable_Oils + Cereals:Obesity + Oilcrops:Pulses + 
    Oilcrops:Vegetable_Oils + Oilcrops:Obesity + 
    Pulses:Vegetable_Oils + Pulses:Obesity +  Vegetable_Oils:Obesity, 
  data=no_unusual_observations)

summary(interaction_terms) #Animal_Fats:Vegetable_oils is statistically significant, this will be added to a new model

new_model_4 <- lm(Deaths~Animal_Fats+Cereals+Oilcrops+Pulses + Vegetable_Oils+
                    Obesity + Animal_Fats:Vegetable_Oils, data=no_unusual_observations)

summary(new_model_4)


# Testing with AIC and BIC 

AIC(new_model_1, new_model_2, new_model_3, new_model_4)
BIC(new_model_1, new_model_2, new_model_3, new_model_4)

# Adding Robust Standard Errors to this new model since we know heteroskedasticity is present 

cov2 <- hccm(new_model_4, type = "hc1")
#Have our model account for those errors. 
new_model_4_adjusted <- coeftest(new_model_4, vcov. = cov2)

tidy(new_model_4_adjusted)
```

As stated above, we did not test for higher powers because the RESET test suggested we should not test our model with quadratic variables. We only tested for interaction terms, and in our model that included all interaction terms we found that the interaction between Animal Fats and Vegetable Oils was statistically significant. This interaction was added to model 3, creating new_model_4. We then went and tested all of our models with AIC and BIC and it was confirmed that new_model_4 had the lowest AIC and was virtually tied in BIC with model 3. This lead us to believe that we had found the best model in model 4. In part 8 we learned that heteroskedacity is present in our data, we took this into cosideration and calculated the robust standard errors for new_model_4, which created new_model_4_adjusted. Again, we see that Pulses becomes statistically significant when accounting for the adjusted standard errors.


### Part 10
Evaluate your model performance (from 9) using cross-validation, and also by dividing your data into the traditional 2/3 training and 1/3 testing samples, to evaluate your out-of-sample performance. Comment on your results. 
```{r}

# split data into 2/3 train 1/3 test
train <- sample(nrow(no_unusual_observations), nrow(no_unusual_observations) * 2/3)
data_train <- no_unusual_observations[train,]
data_test <- no_unusual_observations[-train,]

# do 5-fold cross validation on the training partition
# using model_vif below as placeholder
fitControl <- trainControl(method="cv", number = 5, savePredictions = T)
model_cv <- train(Deaths~Animal_Fats+Cereals+Oilcrops+Pulses + Vegetable_Oils+
                    Obesity + Animal_Fats:Vegetable_Oils, data=data_train,  
                  trControl = fitControl, method = "glm")

model_cv
```

From our cross validation, we see that the RMSE is .036 and the R-squared is .471 within the cross-validation partition.

```{r}

# make predictions on the testing partition
pred <- predict(model_cv, data_test, vcov. = cov2)

# calculate RMSE
RMSE(pred, data_test$Deaths)

```

The RMSE on our test partition from the model created with the training partition is .037, which is in line with the value from above. This is a good sign for the out-of-sample performance of the model.  

### Part 11
Provide a short (1 paragraph) summary of your overall conclusions/findings.

When examining the data overall, there were not any massive outliers that stood out. We found that many of the food categories that were included in tracking fat sources were not statistically or practically significant. Our model was quickly trimmed down from the initial pool of variables that we considered. Through model comparison with AIC and BIC, we found that the best predictors to include were Animal Fats, Cereals, Oilcrops, Pulses, Vegetable Oils, Obesity, and the interaction between Animal Fats and Obesity. Unsurprisingly, Obesity was the most significant predictor in our final model, which is expected given the link between obesity and increased risk to COVID-19. As we went through the parts sequentially, the overall quality of the model improved. We also found that adjusting the errors for heteroskedasticity was a very important step, as Pulses went from insignificant to significant both times that we adjusted.







