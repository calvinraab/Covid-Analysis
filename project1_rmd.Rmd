---
title: "Project 1"
author: "Nancie Kung, Calvin Raab, David Collier and Eitan Shimonovitz"
date: "4/21/2021"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load libraries and import the data. 
```{r}
library(readr)
library(dplyr)
library(leaps)
library(ggplot2)
library(reshape2)
library(scales)
library(corrplot)
library(car)
library(stargazer)

Fat_Supply_Quantity_Data <- read_csv("Fat_Supply_Quantity_Data.csv")
```

Data cleaning process.
```{r}
#select columns that are not filled with zeros
Fat_Supply_data <- Fat_Supply_Quantity_Data %>% select(Country, `Animal Products`, `Animal fats`, `Cereals - Excluding Beer`, Eggs, `Fish, Seafood`, `Fruits - Excluding Wine`, Meat, `Milk - Excluding Butter`, Offals, Oilcrops, Pulses, Spices, `Starchy Roots`, Stimulants, Treenuts, `Vegetal Products`, `Vegetable Oils`, Vegetables, Obesity, Undernourished, Population, Confirmed, Deaths)

Fat_Supply_data <- Fat_Supply_data[Fat_Supply_data$Deaths > 0,]
Fat_Supply_data <- Fat_Supply_data[!is.na(Fat_Supply_data$Deaths),]


Fat_Supply_data$Undernourished[Fat_Supply_data$Undernourished == "<2.5"] <- 2.5
Fat_Supply_data$Undernourished <- as.numeric(Fat_Supply_data$Undernourished)

# replace NAs in Obesity and Undernourished with the median values
Fat_Supply_data$Obesity[is.na(Fat_Supply_data$Obesity)] <- median(Fat_Supply_data$Obesity, na.rm=TRUE)
Fat_Supply_data$Undernourished[is.na(Fat_Supply_data$Undernourished)] <- median(Fat_Supply_data$Undernourished, na.rm=TRUE)

data <- Fat_Supply_data

# Here is a dataset that includes the parameters found in backAIC, along with: Country, Population, Confirmed, and Deaths (Using For Analysis) 
backAICdata.plus <- data_frame(data$`Country`, data$`Animal fats`, data$`Cereals - Excluding Beer`, data$`Fruits - Excluding Wine`, data$`Oilcrops`, data$`Pulses`, data$`Spices`, data$`Starchy Roots`, data$Stimulants, data$Treenuts, data$`Vegetal Products`, data$`Vegetable Oils`, data$Vegetables, data$Obesity, data$Undernourished, data$Population, data$Confirmed, data$Deaths)

names(backAICdata.plus) <- c("Country", "Animal_Fats", "Cereals", "Fruits", "Oilcrops", "Pulses", "Spices", "Starchy_Roots", "Stimulants", "Treenuts", "Vegetal Products", "Vegetable_Oils", "Vegetables", "Obesity", "Undernourished", "Population", "Confirmed", "Deaths")


```

### Part 1 
Provide a descriptive analysis of your variables. This should include histograms and fitted distributions, correlation plot, boxplots, scatterplots, and statistical summaries (e.g., the five-number summary). All figures must include comments.

Columns in dataset:  
* Fat Supply Measures - Average percentage (out of 100) of fat in diet that comes from each category of food
    - Categories included: Animal_Fats, Cereals, Fruits, Oilcrops, Pulses, Spices, Starchy_Roots, Stimulants, Treenuts, Vegetal Products, Vegetable_oils, and Vegetables
    
* Population Health Measures - Percentage of the population that falls into each category
    - Obesity and Undernourished
    
* Population and COVID Measures
    - Population - Population of country
    - Confirmed - Percentage of population with a confirmed positive test for COVID-19
    - Deaths - Percentage of population that died from COVID-19
    

```{r}
# create a boxplot of food categories


# melt the data into long form
fat_data <- melt(backAICdata.plus[,1:13], id = "Country")

# create boxplots
ggplot(fat_data, aes(x = variable, y = value)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(labels = label_percent(scale = 1)) +
  xlab("Food Category") +
  ylab("Percentage of Fat") +
  ggtitle("Boxplot of Percentage of Fat in Diet from Food Categories")
```

From the above boxplots, we can see that Vegetal Products and Vegetable Oils are major sources of fat for all countries, while the average values for other categories are low. We can also see that Oilcrops has a relatively large amount of high outliers compared to other groups.  

```{r}
# correlation plot of all variables
library(corrplot)
corrplot(cor(backAICdata.plus[,-1]), method = "circle")
```

From the above correlation plot, we can see some interesting correlations between some food groups, such as between Vetegal Procucts and Animal Fats. We also see that Obesity and Undernourished are strongly negatively correlated, which makes sense, and that there is a very high correlation between Confirmed Cases and Deaths, which is also to be expected. 


```{r}
# five number summaries for each numeric column

apply(backAICdata.plus[,-1], 2, summary)


```


```{r}
library(ggplot2)
# Percentage of confirmed cases by country - I want to only post the labels of countries that have over a 5% confirmed case level 
ggplot(data = backAICdata.plus, aes(x=Country, y=Confirmed, label= Country)) + geom_point() + scale_x_discrete(guide = guide_axis(check.overlap = TRUE))
```

The above scatterplot details the percentage of confirmed cases in each country. Here it can be seen that the majority of cases lie between zero and 2.5%. From this graphic it can be seen that the highest percentage of covid cases is above 10%. 


### Part 2
Estimate a multiple linear regression model that includes all the main effects only (i.e., no interactions nor higher order terms). We will use this model as a baseline. Comment on the statistical and economic significance of your estimates. Also, make sure to provide an interpretation of your estimates.
```{r}

# To reduce the number of columns in our dataset to a more workable amount, 
# we used backward selection with AIC to pick the predictors we wanted to include. 
# The dataset used for question one used only the selected columns.

model_all <- lm(data$Deaths ~ data$`Animal Products` + data$`Animal fats` + data$`Cereals - Excluding Beer` + data$Eggs + data$`Fish, Seafood` + data$`Fruits - Excluding Wine` + data$Meat + data$`Milk - Excluding Butter` + data$Offals + data$Oilcrops + data$Pulses + data$Spices + data$`Starchy Roots` + data$Stimulants + data$Treenuts + data$`Vegetal Products` + data$`Vegetable Oils` + data$Vegetables + data$Obesity + data$Undernourished + data$Population)

#summary(model_all)

n <- length(data$Deaths)

backAIC <- step(model_all ,direction="backward", data=data)
# Baseline Model
summary(backAIC)

# backBIC <- step(model_all ,direction="backward", data=data, k = log(n))

```

As can be seen in the model output above, all food categories are statistically significant at the $\alpha = .05$ level. Obesity is also a statistically significant predictor, though Undernourished is surprisingly not statistically significant. The magnitude of the estimates for the food categories is roughly the same, with a 1% increase in fat from each food category leading to a .17 - .19 percent change in expected death rate from COVID-19. What is interesting is that Vegetal Products is the only statistically significant predictor with a negative coefficient, while all other food categories are positive. An increase of 1% in population obesity leads to an increase in .001% of expected COVID-19 death rate.  

### Part 3
Identify if there are any outliers, high leverage, and or influential observations worth removing. If so, remove them but justify your reason for doing so and re-estimate your model.
```{r}
library(olsrr)

plot(backAIC)
influencePlot(backAIC, id=list(n=3)) 
ols_plot_resid_lev(backAIC)

# The following observations were identified by both plots as unusual
## high leverage = 120 (Rwanda), 90 (Maldives), 47 (Ethiopia)
## outlier = 103 (New Zealand), 113 (Peru)
## influential = 19 (Bosnia/Herzegovina)

# Remove the unusual observations from the data with slice 
no_highleverage <- backAICdata.plus %>% slice(-c(120,90,47))
no_influential <- backAICdata.plus %>% slice(-19)
no_outlier <- backAICdata.plus %>% slice(-c(103,113))
no_unusual_observations <- backAICdata.plus %>% slice(-c(120,90,47,19,103,113))
```
```{r mylatextable, results = "asis"}
# Create new models without the unusual observations
mod0 <- lm(Deaths~Animal_Fats+Cereals+Fruits+Oilcrops+Pulses+Spices+Starchy_Roots+Stimulants+Treenuts+`Vegetal Products`+Vegetable_Oils+Vegetables+Obesity, data=backAICdata.plus)
mod1 <- lm(Deaths~Animal_Fats+Cereals+Fruits+Oilcrops+Pulses+Spices+Starchy_Roots+Stimulants+Treenuts+`Vegetal Products`+Vegetable_Oils+Vegetables+Obesity, data=no_highleverage)
mod2 <- lm(Deaths~Animal_Fats+Cereals+Fruits+Oilcrops+Pulses+Spices+Starchy_Roots+Stimulants+Treenuts+`Vegetal Products`+Vegetable_Oils+Vegetables+Obesity, data=no_influential)
mod3 <- lm(Deaths~Animal_Fats+Cereals+Fruits+Oilcrops+Pulses+Spices+Starchy_Roots+Stimulants+Treenuts+`Vegetal Products`+Vegetable_Oils+Vegetables+Obesity, data=no_outlier)
mod4 <- lm(Deaths~Animal_Fats+Cereals+Fruits+Oilcrops+Pulses+Spices+Starchy_Roots+Stimulants+Treenuts+`Vegetal Products`+Vegetable_Oils+Vegetables+Obesity, data=no_unusual_observations)

stargazer(mod0, mod1, mod2, mod3, mod4, object.names = TRUE, title = "Regression Model Results", column.labels = c("Original", "No High Leverage", "No Influential", "No Oultlier", "No Unusual Observations"), type = 'latex', header = FALSE, no.space = TRUE, single.row = TRUE, font.size = "small", column.sep.width = "-15pt")
```
```{r}
# New model 
new_model_1 <- lm(Deaths~Animal_Fats+Cereals+Fruits+Oilcrops+Pulses+Spices+Starchy_Roots+Stimulants+Treenuts+`Vegetal Products`+Vegetable_Oils+Vegetables+Obesity, data=no_unusual_observations)
summary(new_model_1)
```


### Part 4
Use Mallows Cp for identifying which terms you will keep in the model (based on part 3) and also use the Boruta algorithm for variable selection. Based on the two results, determine which subset of predictors you will keep.
```{r, message=FALSE}
# Since Mallows CP has a lower number when testing our new_model_1, we will proceed with that model. 
ols_mallows_cp(new_model_1, model_all)
ols_mallows_cp(backAIC, model_all)

# install.packages("Boruta")
library(Boruta)

Bor.res <- Boruta(Deaths~Animal_Fats+Cereals+Fruits+Oilcrops+Pulses+Spices+Starchy_Roots+Stimulants+Treenuts+`Vegetal Products`+Vegetable_Oils+Vegetables+Obesity, data = no_unusual_observations, doTrace = 2 )
# plot(Bor.res, xlab = "", xaxt = "n", main = "Boruta Algorithim")
lz<-lapply(1:ncol(Bor.res$ImpHistory),function(i)
Bor.res$ImpHistory[is.finite(Bor.res$ImpHistory[,i]),i])
names(lz) <- colnames(Bor.res$ImpHistory)
Labels <- sort(sapply(lz,median))
#plot(Bor.res, xlab = "Attributes", main = "Boruta Algorithim")
# Fix labels

# Testing to see which variables we want to remove
attStats(Bor.res)
sorted_vars <- attStats(Bor.res)[order(-attStats(Bor.res)$meanImp),]
print(sorted_vars)
# We will reject: Stimulants, Treenuts, Starchy Roots, Vegetables, Spices, Fruits

# Our New Model 
new_model_2 <- lm(Deaths~Animal_Fats+Cereals+Oilcrops+Pulses+`Vegetal Products`+Vegetable_Oils+Obesity, data=no_unusual_observations)
summary(new_model_2)
```


### Part 5
Test for multicollinearity using VIF on the model from (4) . Based on the test, remove any appropriate variables, and estimate a new regression model based on these findings.
```{r}


vif(backAIC) # We will remove any variable with a VIF over 5 to satisfy collinearity assumption


model_vif <- lm(data$Deaths ~  data$`Cereals - Excluding Beer` + data$Eggs + data$`Fish, Seafood` + data$`Fruits - Excluding Wine` + data$Meat + data$`Milk - Excluding Butter` + data$Offals + data$Oilcrops + data$Pulses + data$Spices + data$`Starchy Roots` + data$Stimulants + data$Treenuts + data$Vegetables + data$Obesity + data$Undernourished + data$Population)

vif(model_vif)

vif(new_model_2) # We will remove any variable with a VIF over 10 to satisfy collinearity assumption

vif(new_model_2) # We will remove any variable with a VIF over 5 to satisfy collinearity assumption


# Remove: Cereals, OilCrops, Vegetal Products, and Vegtable Oils 
new_model_3 <- lm(Deaths~Animal_Fats+Pulses+Obesity, data=no_unusual_observations)

#New MOdel 
summary(new_model_3)

```


### Part 6
For your model in part (5) plot the respective residuals vs. y_hat  and comment on your results.

From the residuals vs fitted plot it can be seen that our residuals appear to spread out the greater our fitted value is. The red smoother runs close to zero which is a good thing. 
```{r}
par(mfrow=c(2,2))
plot(new_model_3)

```


### Part 7
For your model in part (5) perform a RESET test and comment on your results.

Here we tested our model by testing our model against a quadratic. Our result is a p-value of 0.7077 which means we should consider higher order powers. 
```{r}
library(lmtest)

resettest(new_model_3, power = 2, type = "regressor")
```


### Part 8
For your model in part (5) test for heteroskedasticity and comment on your results. If you identify heteroskedasticy, make sure to account for it before moving on to (9).

Below we will test for heteroskedacity using the ncvTest and bptest. 
```{r}
# Non-constant error variance: Ho: variance = constant 
ncvTest(new_model_3) # Reject Ho

# BP test 
bptest(new_model_3) #Reject Ho
```

From the above tests it can be seen that heteroskedacity is present in our data. In order to account for that we will now run our model with robust white standard errors. Here our new standard errors can be found. 
```{r}
cov1 <- hccm(new_model_3, type = "hc1")
#Have our model account for those errors. 
new_model_3_adjusted <- coeftest(new_model_3, vcov. = cov1)
library(broom)
tidy(new_model_3_adjusted)
```

### Part 9
Estimate a model based on all your findings that also includes interaction terms (if appropriate) and if needed, any higher power terms. Comment on the performance of this model compared to your other models. Make sure to use AIC and BIC for model comparison.
```{r}
# Our RESET test suggested there may be an existence of higher power terms, which will be tested here. Specifically we will be checking our variables against them in quadratic form. 

higher_power <- lm(Deaths~Animal_Fats+Pulses+Obesity+I(Animal_Fats^2)+I(Pulses^2)+I(Obesity^2), data=no_unusual_observations)
summary(higher_power) #None of the higher powers are statistically significant

interaction_terms <- lm(Deaths~Animal_Fats+Pulses+Obesity+(Animal_Fats*Pulses)+(Animal_Fats*Obesity)+(Pulses*Obesity), data=no_unusual_observations)

summary(interaction_terms) #Pulses:Obesity is statistically significant, this will be added to a new model

new_model_4 <- lm(Deaths~Animal_Fats+Pulses+Obesity+(Pulses*Obesity), data=no_unusual_observations)

summary(new_model_4)


# Testing with AIC and BIC 
library(broom)
library(AER)
AIC(new_model_1, new_model_2, new_model_3, new_model_4)
BIC(new_model_1, new_model_2, new_model_3, new_model_4)

# Adding Robust Standard Errors to this new model since we know heteroskedasticity is present 

cov2 <- hccm(new_model_4, type = "hc1")
#Have our model account for those errors. 
new_model_4_adjusted <- coeftest(new_model_4, vcov. = cov2)
library(broom)
tidy(new_model_4_adjusted)

```

Above we first tested for higher powers because the RESET test suggested we should test our model with quadratic variables. We found no statistically significant powers. After testing for higher powers we tested for interaction terms. The interaction between Pulses and Obesity was statistically significant so it was added to the model, creating new_model_4. We then went and tested all of our models with AIC and BIC and it was confirmed that new_model_4 had the lowest AIC and BIC, leading us to believe that we had found the best model. In part 8 we learned that heteroskadacity is present in our data, we took this into cosideration and calculated the robust standard errors for new_model_4, which created new_model_4_adjusted. 


### Part 10
Evaluate your model performance (from 9) using cross-validation, and also by dividing your data into the traditional 2/3 training and 1/3 testing samples, to evaluate your out-of-sample performance. Comment on your results. 
```{r}
# install.packages("caret")
# install.packages("lattice")
library(caret)
model_final <- new_model_4 # replace this with the model from 9 once we have it

# split data into 2/3 train 1/3 test
train <- sample(nrow(data), nrow(data) * 2/3)
data_train <- data[train,]
data_test <- data[-train,]

# do 5-fold cross validation on the training partition
# using model_vif below as placeholder
fitControl <- trainControl(method="cv", number = 5, savePredictions = T)
model_cv <- train(Deaths ~  `Cereals - Excluding Beer` + Eggs + `Fish, Seafood` + `Fruits - Excluding Wine` + Meat + `Milk - Excluding Butter` + Offals + Oilcrops + Pulses + Spices + `Starchy Roots` + Stimulants + Treenuts + Vegetables + Obesity + Undernourished + Population, data = data_train, trControl = fitControl, method = "glm")

model_cv

summary(model_cv)

# make predictions on the testing partition
pred <- predict(model_cv, data_test)

# calculate RMSE
RMSE(pred, data_test$Deaths)

```


### Part 11
Provide a short (1 paragraph) summary of your overall conclusions/findings.

Things we may want to say: 
- We learned very quickly that as far as predicting deaths, there were a very limited number of variables that were statistically signifcant, we were cutting down variabels fast. We started with 32 variables and in our first model that was cut to 14. Obesity and Animal_fats appear to have the best prediction power. 

